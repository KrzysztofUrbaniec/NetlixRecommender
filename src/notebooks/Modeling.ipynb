{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:56:36.809952Z",
     "start_time": "2024-06-25T13:56:36.666975Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "package_path = os.path.abspath('../../')\n",
    "if package_path not in sys.path:\n",
    "    sys.path.append(package_path)\n",
    "\n",
    "from surprise import SVD, SVDpp\n",
    "from surprise import NormalPredictor, KNNBaseline\n",
    "from surprise.dump import dump\n",
    "\n",
    "from src.scripts.DataLoader import DataLoader\n",
    "from src.scripts.Data import Data\n",
    "from src.scripts.Algorithm import Algorithm\n",
    "from src.scripts.utils import test_algorithms, grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data in a format required by surprise package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:56:37.710654Z",
     "start_time": "2024-06-25T13:56:37.546250Z"
    }
   },
   "outputs": [],
   "source": [
    "ratings_train_path = '../../data/ratings_training.csv' \n",
    "movies_path = '../../data/movies.csv'\n",
    "\n",
    "dl = DataLoader(ratings_train_path, movies_path)\n",
    "data = dl.load_dataset()\n",
    "\n",
    "ratings = Data(data)\n",
    "\n",
    "# Required to compute Novelty\n",
    "popularity_ranks = dl.get_popularity_ranks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, let's compare the baseline performance of a few algorithms provided by surprise, in order to know, where to focus our attention during the parameter tuning phase. Let's use regular 75/25 train/test split in this initial phase, just to roughly evaluate the performance of particular algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:56:38.903076Z",
     "start_time": "2024-06-25T13:56:38.897700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define candidate algorithms\n",
    "sim_options = {\"name\": \"cosine\", \"user_based\": False}\n",
    "bsl_options = {\"method\": \"sgd\", \"learning_rate\": 0.005}\n",
    "algorithms = {'normal_predictor': NormalPredictor(), 'knn_baseline': KNNBaseline(sim_options=sim_options, bsl_options=bsl_options), 'svd': SVD(), 'svd++': SVDpp()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating performance for: normal_predictor\n",
      "Training the model and making predictions...\n",
      "Computing RMSE and MAE...\n",
      "RMSE: 1.4570\n",
      "MAE:  1.1683\n",
      "\n",
      "Done.\n",
      "\n",
      "Evaluating performance for: knn_baseline\n",
      "Training the model and making predictions...\n",
      "Estimating biases using sgd...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing RMSE and MAE...\n",
      "RMSE: 0.8964\n",
      "MAE:  0.6979\n",
      "\n",
      "Done.\n",
      "\n",
      "Evaluating performance for: svd\n",
      "Training the model and making predictions...\n",
      "Computing RMSE and MAE...\n",
      "RMSE: 0.8853\n",
      "MAE:  0.6904\n",
      "\n",
      "Done.\n",
      "\n",
      "Evaluating performance for: svd++\n",
      "Training the model and making predictions...\n",
      "Computing RMSE and MAE...\n",
      "RMSE: 0.8740\n",
      "MAE:  0.6756\n",
      "\n",
      "Done.\n",
      "\n",
      "                    RMSE     MAE\n",
      "svd++             0.8740  0.6756\n",
      "svd               0.8853  0.6904\n",
      "knn_baseline      0.8964  0.6979\n",
      "normal_predictor  1.4570  1.1683\n"
     ]
    }
   ],
   "source": [
    "eval_df = test_algorithms(algorithms, ratings, verbose=True)\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems, that SVD++ achieves the best performance on the validation set out of all four candidates (using default hyperparameters), in terms of both RMSE and MAE (actually, NormalPredictor randomly generates predictions from a normal distribution, so it was just a reference point)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initial algorithm selection, let's try to tune its parameters to, hopefully, further reduce prediction error of the model. Let's use 3-fold cross-validation, not to spend too much time on this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the search...\n",
      "Extracting best parameters and scores...\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "svdpp_param_grid = {\n",
    "    'n_factors': [100,125],\n",
    "    'n_epochs': [20],\n",
    "    'lr_all': [0.005, 0.05, 0.5],\n",
    "    'reg_all': [0.02, 0.05, 0.1]\n",
    "}\n",
    "best_params, best_scores = grid_search(data, SVDpp, svdpp_param_grid, joblib_verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'rmse': {'n_factors': 125, 'n_epochs': 20, 'lr_all': 0.005, 'reg_all': 0.05}, 'mae': {'n_factors': 125, 'n_epochs': 20, 'lr_all': 0.005, 'reg_all': 0.05}}\n",
      "Best scores: {'rmse': 0.876660358692631, 'mae': 0.6847703061233724}\n"
     ]
    }
   ],
   "source": [
    "print(f'Best params: {best_params}')\n",
    "print(f'Best scores: {best_scores}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-centered validation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, RMSE, MAE or any other accuracy metric alone is not a good indicator of performance of a recommendation model (this also turned out to be the case after original Netflix Prize). Recommendation systems are living things and the ultimate validation of their performance is provided by the customers and their shopping decisions. However, what we can do to gain more information about model's behavior, is to use user-centric metrics, like:\n",
    "- hit rate (HR) (are predicted items relevant to a user?)\n",
    "- cumulative hit rate (cHR) (does the model predict items, which user really likes?)\n",
    "- average reciprocal hit rank (ARHR)\n",
    "- user coverage (what percentage of users have at least one good recommendation, in terms of predicted rating?)\n",
    "- diversity (how diverse/dissimilar are the items recommended to users, on average?)\n",
    "- novelty (how many non-mainstream items are recommended to users, on average?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute these metrics, we will need a list of predicted items for each user, which can be obtained with help of leave-one-out cross-validation. User coverage, diversity and novelty need also information about number of users, similarity scores between movies and movie popularity ranks, respectively. Let's also serialize the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and params\n",
    "with open('../models/svd_final_params.json', 'w') as file:\n",
    "    json.dump(best_params,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/svd_final_params.json', 'r') as file:\n",
    "    best_params = json.load(file)\n",
    "selected_algo = SVDpp(**best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing accuracy metrics...\n",
      "\n",
      "Computing user-centered metrics...\n",
      "Fitting the algorithm to the leave-one-out CV trainset...\n",
      "Making predictions for the leave-one-out CV testset...\n",
      "Making precictions for the anti testset...\n",
      "Generating top N recommendations for all users...\n",
      "Computing hit rate...\n",
      "Computing cumulative hit rate...\n",
      "Computing average reciprocal hit rank...\n",
      "Computing diversity...\n",
      "Computing user coverage...\n",
      "Computing novelty...\n",
      "\n",
      "RMSE = 0.870192\n",
      "MSE = 0.679322\n",
      "HR = 0.028933\n",
      "cHR = 0.044606\n",
      "ARHR = 0.008582\n",
      "Diversity = 0.036372\n",
      "User Coverage = 0.943200\n",
      "Novelty = 0.000000\n",
      "\n",
      "Legend:\n",
      "\n",
      "HR:        Hit Rate; how often we are able to recommend a left-out rating. Higher is better.\n",
      "cHR:       Cumulative Hit Rate; hit rate, confined to ratings above a certain threshold. Higher is better.\n",
      "ARHR:      Average Reciprocal Hit Rank - Hit rate that takes the ranking into account. Higher is better.\n",
      "Diversity: 1-S, where S is the average similarity score between every possible pair of recommendations\n",
      "           for a given user. Higher means more diverse.\n",
      "User Coverage:  Ratio of users for whom recommendations above a certain threshold exist. Higher is better.\n",
      "Novelty:   Average popularity rank of recommended items. Higher means more novel.\n"
     ]
    }
   ],
   "source": [
    "algo = Algorithm(selected_algo, ratings, popularity_ranks)\n",
    "# Compute accuracy metrics using single trainset and validation set\n",
    "# Compute user-centered metrics using leave-one-out cross-validation\n",
    "algo.evaluate(compute_accuracy_only=False, verbose=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until we compare these metrics for a few different models, we cannot really say, whether the values we obtained are high or low. What's clearly noticeable is that novelty is equal to 0. Novelty is computed as a sum of popularity ranks of recommended movies for all users divided by the total number of recommended items. This means, that the model recommends only items with low ranks, i.e. the most popular items. Also, user coverage seems to be quite high (~94%), but it should be noted that it was computed on a small sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ultimate goal of a recommendation model/system is to, well, recommend different items, which in our case are movies. Let's also serialize the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on full dataset separately\n",
    "algo.fit_with_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dump has been saved as file ../models/svd_final.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "dump('../models/svd_final.pkl', algo=algo, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations:\n",
      "The West Wing: Season 2: 4.577\n",
      "House M.D.: Season 1: 4.555\n",
      "Coupling: Season 2: 4.554\n",
      "Lord of the Rings: The Two Towers: Extended Edition: 4.545\n",
      "Finding Nemo (Widescreen): 4.523\n",
      "24: Season 1: 4.51\n",
      "The Lord of the Rings: The Fellowship of the Ring: Extended Edition: 4.503\n",
      "Sex and the City: Season 3: 4.496\n",
      "CSI: Season 2: 4.473\n",
      "Six Feet Under: Season 2: 4.457\n"
     ]
    }
   ],
   "source": [
    "# Sample recommendations\n",
    "recs = algo.generate_recommendations('1732491',topN=10)\n",
    "print('Recommendations:')\n",
    "for movie_id, rating in recs:\n",
    "    print(f'{dl.get_movie_name(int(movie_id))}: {np.round(rating,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model's performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.080824918502178\n",
      "MAE: 0.9066404037474705\n"
     ]
    }
   ],
   "source": [
    "ratings_test_path = '../../data/ratings_test.csv'\n",
    "testset = pd.read_csv(ratings_test_path)\n",
    "algo.validate_test(testset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
