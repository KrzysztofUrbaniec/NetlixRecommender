{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:56:36.809952Z",
     "start_time": "2024-06-25T13:56:36.666975Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "package_path = os.path.abspath('../../src')\n",
    "if package_path not in sys.path:\n",
    "    sys.path.append(package_path)\n",
    "\n",
    "from surprise import SVD, SVDpp\n",
    "from surprise import NormalPredictor, KNNBaseline\n",
    "\n",
    "from scripts.DataLoader import DataLoader\n",
    "from scripts.Data import Data\n",
    "from scripts.Algorithm import Algorithm\n",
    "from scripts.utils import test_algorithms, grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data in a format required by surprise package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:56:37.710654Z",
     "start_time": "2024-06-25T13:56:37.546250Z"
    }
   },
   "outputs": [],
   "source": [
    "ratings_train_path = '../../data/ratings_training.csv' \n",
    "movies_path = '../../data/movies.csv'\n",
    "\n",
    "dl = DataLoader(ratings_train_path, movies_path)\n",
    "data = dl.load_dataset()\n",
    "\n",
    "ratings = Data(data)\n",
    "\n",
    "# Required to compute Novelty\n",
    "popularity_ranks = dl.get_popularity_ranks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, let's compare the baseline performance of a few algorithms provided by surprise, in order to know, where to focus our attention during the parameter tuning phase. Let's use regular 75/25 train/test split in this initial phase, just to roughly evaluate the performance of particular algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:56:38.903076Z",
     "start_time": "2024-06-25T13:56:38.897700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define candidate algorithms\n",
    "sim_options = {\"name\": \"cosine\", \"user_based\": False}\n",
    "bsl_options = {\"method\": \"sgd\", \"learning_rate\": 0.005}\n",
    "algorithms = {'normal_predictor': NormalPredictor(), 'knn_baseline': KNNBaseline(sim_options=sim_options, bsl_options=bsl_options), 'svd': SVD(), 'svd++': SVDpp()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating performance for: normal_predictor\n",
      "Training the model and making predictions...\n",
      "Computing RMSE and MAE...\n",
      "RMSE: 1.4403\n",
      "MAE:  1.1533\n",
      "\n",
      "Done.\n",
      "\n",
      "Evaluating performance for: svd\n",
      "Training the model and making predictions...\n",
      "Computing RMSE and MAE...\n",
      "RMSE: 0.9197\n",
      "MAE:  0.7202\n",
      "\n",
      "Done.\n",
      "\n",
      "Evaluating performance for: svd++\n",
      "Training the model and making predictions...\n",
      "Computing RMSE and MAE...\n",
      "RMSE: 0.9037\n",
      "MAE:  0.7014\n",
      "\n",
      "Done.\n",
      "\n",
      "                    RMSE     MAE\n",
      "svd++             0.9037  0.7014\n",
      "svd               0.9197  0.7202\n",
      "normal_predictor  1.4403  1.1533\n"
     ]
    }
   ],
   "source": [
    "eval_df = test_algorithms(algorithms, ratings)\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems, that SVD++ achieves the best performance on the validation set out of all four candidates (using default hyperparameters), in terms of both RMSE and MAE (actually, NormalPredictor randomly generates predictions from a normal distribution, so it was just a reference point)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initial algorithm selection, let's try to tune its parameters to, hopefully, further reduce prediction error of the model. Let's use 3-fold cross-validation, not to spend too much time on this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   6 | elapsed:  4.1min remaining:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:  4.3min finished\n"
     ]
    }
   ],
   "source": [
    "svdpp_param_grid = {\n",
    "    'n_factors': [50,100,200],\n",
    "    'n_epochs': [20],\n",
    "    'lr_all': [0.005, 0.05, 0.5],\n",
    "    'reg_all': [0.02, 0.05, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "best_params, best_scores = grid_search(ratings, SVDpp, svdpp_param_grid, joblib_verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-centered validation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, RMSE, MAE or any other accuracy metric alone is not a good indicator of performance of a recommendation model (this also turned out to be the case after original Netflix Prize). Recommendation systems are living things and the ultimate validation of their performance is provided by the customers and their shopping decisions. However, what we can do to gain more information about model's behavior, is to use user-centric metrics, like:\n",
    "- hit rate (HR) (are predicted items relevant to a user?)\n",
    "- cumulative hit rate (cHR) (does the model predict items, which user really likes?)\n",
    "- average reciprocal hit rank (ARHR)\n",
    "- user coverage (what percentage of users have at least one good recommendation, in terms of predicted rating?)\n",
    "- diversity (how diverse/dissimilar are the items recommended to users, on average?)\n",
    "- novelty (how many non-mainstream items are recommended to users, on average?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute these metrics, we will need a list of predicted items for each user, which can be obtained with help of leave-one-out cross-validation. User coverage, diversity and novelty need also information about number of users, similarity scores between movies and movie popularity ranks, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = Algorithm(SVDpp(**best_params), ratings, popularity_ranks)\n",
    "# Compute accuracy metrics using single trainset and validation set\n",
    "# Compute user-centered metrics using leave-one-out cross-validation\n",
    "algo.evaluate(print_accuracy_only=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until we compare these metrics for a few different models, we cannot really say, whether the values we obtained are high or low. What's clearly noticeable is that novelty is equal to 0. This is to be expected, because this metric is computed using movie popularity ranks (the more ratings a movie has, the lower is its rank) and I've removed infrequently rated movies before. Also, user coverage seems to be quite high (~96%), but it should be noted that it was computed on a small sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ultimate goal of a recommendation model/system is to, well, recommend different items, which in our case are movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = algo.generate_recommendations('410917',topN=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations:\n",
      "The Simpsons: Season 4: 4.766\n",
      "Monty Python's Life of Brian: 4.708\n",
      "Simpsons Gone Wild: 4.664\n",
      "The Office Special: 4.659\n",
      "Band of Brothers: 4.579\n",
      "The Third Man: 4.55\n",
      "Sunset Boulevard: 4.545\n",
      "The Simpsons: Season 2: 4.511\n",
      "Michael Moore's The Awful Truth: Season 1: 4.51\n",
      "Eternal Sunshine of the Spotless Mind: 4.507\n"
     ]
    }
   ],
   "source": [
    "print('Recommendations:')\n",
    "for movie_id, rating in recs:\n",
    "    print(f'{dl.get_movie_name(int(movie_id))}: {np.round(rating,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model's performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0900652100508552\n",
      "MAE: 0.9073789256477611\n"
     ]
    }
   ],
   "source": [
    "ratings_test_path = '../../data/ratings_test.csv'\n",
    "testset = pd.read_csv(ratings_test_path)\n",
    "algo.validate_test(testset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
