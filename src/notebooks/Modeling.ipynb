{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:56:36.809952Z",
     "start_time": "2024-06-25T13:56:36.666975Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "package_path = os.path.abspath('../../src')\n",
    "if package_path not in sys.path:\n",
    "    sys.path.append(package_path)\n",
    "\n",
    "from surprise import SVD, SVDpp\n",
    "from surprise import NormalPredictor, KNNBaseline\n",
    "\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.dump import dump\n",
    "\n",
    "from scripts.DataLoader import DataLoader\n",
    "from scripts.Metrics import Metrics\n",
    "from scripts.utils import test_algorithms, grid_search, predict_top_n_recs, compute_topN_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data in a format required by surprise package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:56:37.710654Z",
     "start_time": "2024-06-25T13:56:37.546250Z"
    }
   },
   "outputs": [],
   "source": [
    "ratings_train_path = '../../data/ratings_training.csv' \n",
    "movies_path = '../../data/movies.csv'\n",
    "\n",
    "dl = DataLoader(ratings_train_path, movies_path)\n",
    "ratings = dl.load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, let's compare the baseline performance of a few algorithms provided by surprise, in order to know, where to focus our attention during the parameter tuning phase. Let's use regular 75/25 train/test split in this initial phase, just to roughly evaluate the performance of particular algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:56:38.903076Z",
     "start_time": "2024-06-25T13:56:38.897700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define candidate algorithms\n",
    "# algorithms = {'normal_predictor': NormalPredictor(), 'knn_baseline': KNNBaseline(), 'svd': SVD(), 'svd++': SVDpp()}\n",
    "algorithms = {'normal_predictor': NormalPredictor(), 'svd': SVD(), 'svd++': SVDpp()}\n",
    "\n",
    "# Split the data once, not to do it each time a new model is trained\n",
    "train, test = train_test_split(ratings, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating performance for: normal_predictor\n",
      "Training the model and making predictions...\n",
      "Computing RMSE and MAE...\n",
      "RMSE: 1.4424\n",
      "MAE:  1.1551\n",
      "\n",
      "Done.\n",
      "\n",
      "Evaluating performance for: svd\n",
      "Training the model and making predictions...\n",
      "Computing RMSE and MAE...\n",
      "RMSE: 0.9217\n",
      "MAE:  0.7215\n",
      "\n",
      "Done.\n",
      "\n",
      "Evaluating performance for: svd++\n",
      "Training the model and making predictions...\n",
      "Computing RMSE and MAE...\n",
      "RMSE: 0.9025\n",
      "MAE:  0.7000\n",
      "\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_df = test_algorithms(algorithms, train, test)\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems, that SVD++ achieves the best performance on the validation set out of all four candidates (using default hyperparameters), in terms of both RMSE and MAE (actually, NormalPredictor randomly generates predictions from a normal distribution, so it was just a reference point)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initial algorithm selection, let's try to tune its parameters to, hopefully, further reduce prediction error of the model. Let's use 3-fold cross-validation, not to spend too much time on this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   6 | elapsed:  4.1min remaining:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:  4.3min finished\n"
     ]
    }
   ],
   "source": [
    "svdpp_param_grid = {\n",
    "    'n_factors': [20,50,100,200],\n",
    "    'n_epochs': [20,40],\n",
    "    'lr_all': [0.005, 0.007, 0.009],\n",
    "    'reg_all': [0.02, 0.05, 0.1, 0.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting best parameters and scores...\n",
      "\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   20.6s finished\n"
     ]
    }
   ],
   "source": [
    "params, scroes = grid_search(ratings, SVDpp, {'n_factors': [10]}, joblib_verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-centered validation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, RMSE, MAE or any other accuracy metric alone is not a good indicator of performance of a recommendation model (this also turned out to be the case after original Netflix Prize). Recommendation systems are living things and the ultimate validation of their performance is provided by the customers and their shopping decisions. However, what we can do to gain more information about model's behavior, is to use user-centric metrics, like:\n",
    "- hit rate (HR) (are predicted items relevant to a user?)\n",
    "- cumulative hit rate (cHR) (does the model predict items, which user really likes?)\n",
    "- average reciprocal hit rank (ARHR)\n",
    "- user coverage (what percentage of users have at least one good recommendation, in terms of predicted rating?)\n",
    "- diversity (how diverse/dissimilar are the items recommended to users, on average?)\n",
    "- novelty (how many non-mainstream items are recommended to users, on average?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute these metrics, we will need a list of predicted items for each user, which can be obtained with help of leave-one-out cross-validation. User coverage, diversity and novelty need also information about number of users, similarity scores between movies and movie popularity ranks, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "svdpp = SVDpp()\n",
    "left_out_predictions, topN = predict_top_n_recs(svdpp, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For novelty\n",
    "popularity_ranks = dl.getPopularityRanks()\n",
    "\n",
    "# For diversity\n",
    "full_training_set = ratings.build_full_trainset()\n",
    "sim_options = {'name': 'cosine', 'user_based': False}\n",
    "simsAlgo = KNNBaseline(sim_options=sim_options, verbose=False)\n",
    "simsAlgo.fit(full_training_set)\n",
    "\n",
    "# For user coverage\n",
    "n_users = len(np.unique([int(row[0]) for row in ratings.raw_ratings]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR = 0.030736\n",
      "cHR = 0.045129\n",
      "ARHR = 0.009693\n",
      "Diversity = 0.029402\n",
      "User Coverage = 0.956295\n",
      "Novelty = 0.000000\n",
      "\n",
      "Legend:\n",
      "\n",
      "HR:        Hit Rate; how often we are able to recommend a left-out rating. Higher is better.\n",
      "cHR:       Cumulative Hit Rate; hit rate, confined to ratings above a certain threshold. Higher is better.\n",
      "ARHR:      Average Reciprocal Hit Rank - Hit rate that takes the ranking into account. Higher is better.\n",
      "Diversity: 1-S, where S is the average similarity score between every possible pair of recommendations\n",
      "           for a given user. Higher means more diverse.\n",
      "User Coverage:  Ratio of users for whom recommendations above a certain threshold exist. Higher is better.\n",
      "Novelty:   Average popularity rank of recommended items. Higher means more novel.\n"
     ]
    }
   ],
   "source": [
    "metrics = compute_topN_metrics(left_out_predictions, topN, popularity_ranks, simsAlgo, n_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until we compare these metrics for a few different models, we can really say, whether the values we obtained are high or low. What's clearly noticeable is that novelty is equal to 0. This is to be expected, because this metric is computed using movie popularity ranks (the more ratings a movie has, the lower is its rank) and I've removed infrequently rated movies before. Also, user coverage seems to be quite high (~96%), but it should be noted that it was computed on a small sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprise provides a wrapper around Pickle to serialize fitted models. Let's save the final model, so we can validate it on a test set in a separate notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dump has been saved as file ../models/svd_final\n"
     ]
    }
   ],
   "source": [
    "dump('../models/svd_final', algo=svdpp, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ultimate goal of a reccomentation model/system is to, well, recommend different items, which in our case are movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
